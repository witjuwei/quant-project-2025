"""
é‚¢ä¸è¡Œï½œç­–ç•¥åˆ†äº«ä¼š
é€‰å¸ç­–ç•¥æ¡†æ¶ğ“Ÿğ“»ğ“¸

ç‰ˆæƒæ‰€æœ‰ Â©ï¸ é‚¢ä¸è¡Œ
å¾®ä¿¡: xbx1717

æœ¬ä»£ç ä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨ï¼Œæœªç»æˆæƒä¸å¾—å¤åˆ¶ã€ä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚

Author: é‚¢ä¸è¡Œ
"""
import hashlib

import gc
import shutil
import warnings

import numba as nb
import numpy as np
import pandas as pd

from config import stable_symbol, swap_path, spot_path
from pathlib import Path
from typing import Dict, List

from core.model.backtest_config import BacktestConfig
from core.utils.log_kit import logger
from core.utils.path_kit import get_file_path

warnings.filterwarnings('ignore')


# =====ç­–ç•¥ç›¸å…³å‡½æ•°
def del_insufficient_data(symbol_candle_data) -> Dict[str, pd.DataFrame]:
    """
    åˆ é™¤æ•°æ®é•¿åº¦ä¸è¶³çš„å¸ç§ä¿¡æ¯

    :param symbol_candle_data:
    :return
    """
    # ===åˆ é™¤æˆäº¤é‡ä¸º0çš„çº¿æ•°æ®ã€kçº¿æ•°ä¸è¶³çš„å¸ç§
    symbol_list = list(symbol_candle_data.keys())
    for symbol in symbol_list:
        # åˆ é™¤ç©ºçš„æ•°æ®
        if symbol_candle_data[symbol] is None or symbol_candle_data[symbol].empty:
            del symbol_candle_data[symbol]
            continue
        # åˆ é™¤è¯¥å¸ç§æˆäº¤é‡=0çš„kçº¿
        # symbol_candle_data[symbol] = symbol_candle_data[symbol][symbol_candle_data[symbol]['volume'] > 0]

    return symbol_candle_data


def ignore_error(anything):
    return anything


def load_min_qty(file_path: Path) -> (int, Dict[str, int]):
    # è¯»å–min_qtyæ–‡ä»¶å¹¶è½¬ä¸ºdictæ ¼å¼
    min_qty_df = pd.read_csv(file_path, encoding='gbk')
    min_qty_df['æœ€å°ä¸‹å•é‡'] = -np.log10(min_qty_df['æœ€å°ä¸‹å•é‡']).round().astype(int)
    default_min_qty = min_qty_df['æœ€å°ä¸‹å•é‡'].max()
    min_qty_df.set_index('å¸ç§', inplace=True)
    min_qty_dict = min_qty_df['æœ€å°ä¸‹å•é‡'].to_dict()

    return default_min_qty, min_qty_dict


def is_trade_symbol(symbol, black_list, white_list) -> bool:
    """
    è¿‡æ»¤æ‰ä¸èƒ½ç”¨äºäº¤æ˜“çš„å¸ç§ï¼Œæ¯”å¦‚ç¨³å®šå¸ã€éUSDTäº¤æ˜“å¯¹ï¼Œä»¥åŠä¸€äº›æ æ†å¸
    :param symbol: äº¤æ˜“å¯¹
    :param black_list: é»‘åå•
    :param white_list: ç™½åå•
    :return: æ˜¯å¦å¯ä»¥è¿›å…¥äº¤æ˜“ï¼ŒTrueå¯ä»¥å‚ä¸é€‰å¸ï¼ŒFalseä¸å‚ä¸
    """
    if white_list:
        if symbol in white_list:
            return True
        else:
            return False

    # ç¨³å®šå¸å’Œé»‘åå•å¸ä¸å‚ä¸
    if not symbol or not symbol.endswith('USDT') or symbol in black_list:
        return False

    # ç­›é€‰æ æ†å¸
    base_symbol = symbol.upper().replace('-USDT', 'USDT')[:-4]
    if base_symbol.endswith(('UP', 'DOWN', 'BEAR', 'BULL')) and base_symbol != 'JUP' or base_symbol in stable_symbol:
        return False
    else:
        return True


def align_spot_swap_mapping(df, column_name, n):
    """
    å¤„ç†spotå’Œswapçš„æ˜ å°„å…³ç³»
    :param df: åŸå§‹kçº¿æ•°æ®
    :param column_name: éœ€è¦å¤„ç†çš„åˆ—
    :param n: éœ€è¦è°ƒæ•´æ˜ å°„çš„å‘¨æœŸæ•°é‡
    :return: è°ƒæ•´å¥½çš„kçº¿æ•°æ®
    """
    # åˆ›å»ºæ–°ç»„æ ‡è¯†åˆ—
    df['is_new_group'] = (df[column_name].ne('') & df[column_name].shift().eq('')).astype(int)
    # ç´¯ç§¯æ±‚å’Œç”Ÿæˆç»„å·
    df['group'] = df['is_new_group'].cumsum()
    # å°†ç©ºå­—ç¬¦ä¸²å¯¹åº”çš„ç»„å·è®¾ä¸ºNaN
    df.loc[df['symbol_swap'].eq(''), 'group'] = np.nan
    # é€šè¿‡ groupby æ·»åŠ  grp_seq
    df['grp_seq'] = df.groupby('group').cumcount()
    # è¿‡æ»¤æ¡ä»¶å¹¶ä¿®æ”¹å‰ n è¡Œ
    df.loc[df['grp_seq'] < n, column_name] = ''

    # åˆ é™¤è¾…åŠ©åˆ—
    df.drop(columns=['is_new_group', 'group', 'grp_seq'], inplace=True)

    return df


def load_spot_and_swap_data(conf: BacktestConfig) -> (pd.DataFrame, pd.DataFrame):
    """
    åŠ è½½ç°è´§å’Œåˆçº¦æ•°æ®
    :param conf: å›æµ‹é…ç½®
    :return:
    """
    logger.debug('ğŸ§¹ æ¸…ç†æ•°æ®ç¼“å­˜')
    cache_path = get_file_path('data', 'cache', as_path_type=True)
    if cache_path.exists():
        shutil.rmtree(cache_path)

    logger.debug('ğŸ’¿ åŠ è½½ç°è´§å’Œåˆçº¦æ•°æ®...')
    # è¯»å…¥åˆçº¦æ•°æ®
    symbol_swap_candle_data = pd.read_pickle(swap_path)
    # è¿‡æ»¤æ‰ä¸èƒ½ç”¨äºäº¤æ˜“çš„å¸ç§
    symbol_swap_candle_data = {
        k: align_spot_swap_mapping(v, 'symbol_spot', conf.min_kline_num)
        for k, v in symbol_swap_candle_data.items()
        if is_trade_symbol(k, conf.black_list, conf.white_list)
    }

    # è¿‡æ»¤æ‰æ•°æ®ä¸è¶³çš„å¸ç§
    all_candle_df_list = list(del_insufficient_data(symbol_swap_candle_data).values())
    all_symbol_list = set(symbol_swap_candle_data.keys())

    # è¯»å…¥ç°è´§æ•°æ®
    if conf.is_use_spot:
        symbol_spot_candle_data = pd.read_pickle(spot_path)
        # è¿‡æ»¤æ‰ä¸èƒ½ç”¨äºäº¤æ˜“çš„å¸ç§
        symbol_spot_candle_data = {
            k: align_spot_swap_mapping(v, 'symbol_swap', conf.min_kline_num)
            for k, v in symbol_spot_candle_data.items()
            if is_trade_symbol(k, conf.black_list, conf.white_list)
        }

        # è¿‡æ»¤æ‰æ•°æ®ä¸è¶³çš„å¸ç§
        all_candle_df_list = all_candle_df_list + list(del_insufficient_data(symbol_spot_candle_data).values())
        all_symbol_list = list(all_symbol_list | set(symbol_spot_candle_data.keys()))
        del symbol_spot_candle_data

    # ä¿å­˜æ•°æ®
    pkl_path = get_file_path('data', 'cache', 'all_candle_df_list.pkl')
    pd.to_pickle(all_candle_df_list, pkl_path)

    del symbol_swap_candle_data
    del all_candle_df_list

    gc.collect()

    return tuple(all_symbol_list)  # èŠ‚çœå†…å­˜ï¼ŒåŒ…è£…æˆtuple


@nb.njit
def super_fast_groupby_and_rank(values, if_reverse):
    """
    å®ç°é€»è¾‘ï¼š
    1. å¯¹æ¯ä¸ªç»„è¿›è¡Œæ’åºã€‚
    2. æ‰¾åˆ°æ¯ä¸ªç»„å†…ç›¸åŒå€¼çš„å…ƒç´ ã€‚
    3. ä¸ºè¿™äº›ç›¸åŒå€¼çš„å…ƒç´ åˆ†é…ç›¸åŒçš„æœ€å°æ’åã€‚

    :param values:
    :param if_reverse:
    :return:
    """
    # å¯¹ 'candle_begin_time' è¿›è¡Œæ’åºï¼Œç¡®ä¿åˆ†ç»„æ—¶æ˜¯æœ‰åºçš„
    sorted_idx = np.lexsort((values[:, 0], values[:, 1]))
    sorted_values = values[sorted_idx]

    # åˆ†ç»„æ“ä½œ
    unique_times, group_indices = np.unique(sorted_values[:, 1], return_index=True)

    # åˆå§‹åŒ–æ’åæ•°ç»„
    ranks = np.empty_like(sorted_values[:, 0], dtype=np.float64)

    # é€ç»„å¤„ç†å¹¶è®¡ç®—æ’å
    for i in range(len(group_indices)):
        start_idx = group_indices[i]
        end_idx = group_indices[i + 1] if i + 1 < len(group_indices) else len(sorted_values)
        group_values = sorted_values[start_idx:end_idx, 0]

        # ä½¿ç”¨ numpy çš„ argsort æ¥æ’åºï¼Œå¹¶è®¡ç®—æ’å
        sorted_order = np.argsort(group_values)
        if if_reverse:
            sorted_order = sorted_order[::-1]

        ranks[start_idx:end_idx] = np.argsort(sorted_order) + 1  # è®¡ç®—æ’åï¼Œ+1 æ˜¯ä¸ºäº†ä» 1 å¼€å§‹

    # è¿˜åŸæ’åç»“æœåˆ°åŸå§‹é¡ºåº
    final_ranks = np.empty_like(ranks)
    final_ranks[sorted_idx] = ranks

    return final_ranks


def check_md5(hash_file: Path, factor_df_md5: str) -> bool:
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨hashæ ¡éªŒæ–‡ä»¶
    if hash_file.exists():
        hash_val = hash_file.read_text(encoding='utf8')
        return factor_df_md5 == hash_val

    return False


def save_md5(hash_file: Path, factor_df_md5: str) -> None:
    hash_file.write_text(factor_df_md5, encoding='utf8')


def calc_factor_md5(df: pd.DataFrame, data_size: int = 100) -> str:
    hash_object = hashlib.md5()
    # å°†æ¯ä¸ªå—è½¬æ¢ä¸ºCSVæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¸åŒ…å«ç´¢å¼•å’Œåˆ—å
    chunk_str = df.tail(data_size).to_csv(index=False, header=False).encode('utf-8')
    # æ›´æ–°å“ˆå¸Œå¯¹è±¡çš„æ•°æ®
    hash_object.update(chunk_str)
    factor_df_md5 = hash_object.hexdigest()

    return factor_df_md5


def save_performance_df_csv(conf: BacktestConfig, **kwargs):
    for name, df in kwargs.items():
        file_path = conf.get_result_folder() / f'{name}.csv'
        df.to_csv(file_path, encoding='utf-8-sig')


# ===============================================================================================================
# é¢å¤–æ•°æ®æº
# ===============================================================================================================
def merge_data(df: pd.DataFrame, data_name: str, save_cols: List[str], symbol: str = '') -> dict[str, pd.Series]:
    """
    å¯¼å…¥æ•°æ®ï¼Œæœ€ç»ˆåªè¿”å›å¸¦æœ‰åŒindexçš„æ•°æ®
    :param df: ï¼ˆåªè¯»ï¼‰åŸå§‹çš„è¡Œæƒ…æ•°æ®ï¼Œä¸»è¦æ˜¯å¯¹é½æ•°æ®ç”¨çš„
    :param data_name: æ•°æ®ä¸­å¿ƒä¸­çš„æ•°æ®è‹±æ–‡å
    :param save_cols: éœ€è¦ä¿å­˜çš„åˆ—
    :param symbol: å¸ç§
    :return: åˆå¹¶åçš„æ•°æ®
    """
    import core.data_bridge as db
    from config import data_source_dict

    func_name, file_path = data_source_dict[data_name]

    if hasattr(db, func_name):
        extra_df: pd.DataFrame = getattr(db, func_name)(file_path, df, save_cols, symbol)
    else:
        print(f'âš ï¸ æœªå®ç°æ•°æ®æºï¼š{data_name}')
        return {col: pd.Series([np.nan] * len(df)) for col in save_cols}

    if extra_df is None or extra_df.empty:
        return {col: pd.Series([np.nan] * len(df)) for col in save_cols}

    return {col: extra_df[col] for col in save_cols}


def check_cfg():
    """
    æ£€æŸ¥ data_source_dict é…ç½®
    æ£€æŸ¥åŠ è½½æ•°æ®æºå‡½æ•°æ˜¯å¦å­˜åœ¨
    æ£€æŸ¥æ•°æ®æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    :return:
    """
    import core.data_bridge as db
    from config import data_source_dict
    for key, value in data_source_dict.items():
        func_name, file_path = value
        if not hasattr(db, func_name):
            raise Exception(f"ã€{key}ã€‘åŠ è½½æ•°æ®æºæ–¹æ³•æœªå®ç°ï¼š{func_name}")

        if not (file_path and Path(file_path).exists()):
            raise Exception(f"ã€{key}ã€‘æ•°æ®æºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")

    print('âœ… data_source_dict é…ç½®æ£€æŸ¥é€šè¿‡')


def check_factor(factors: list):
    """
    æ£€æŸ¥å› å­ä¸­çš„é…ç½®
    æ£€æŸ¥æ˜¯å¦æœ‰ extra_data_dict
    æ£€æŸ¥ extra_data_dict ä¸­çš„æ•°æ®æºæ˜¯å¦åœ¨ data_source_dict ä¸­

    å› å­ä¸­çš„å¤–éƒ¨æ•°æ®ä½¿ç”¨æ¡ˆä¾‹:

    extra_data_dict = {
        'coin-cap': ['circulating_supply']
    }

    :param factors:
    :return:
    """
    from core.utils.factor_hub import FactorHub
    for factor_name in factors:
        factor = FactorHub.get_by_name(factor_name)  # è·å–å› å­ä¿¡æ¯
        if not (hasattr(factor, 'extra_data_dict') and factor.extra_data_dict):
            raise Exception(f"æœªæ‰¾åˆ°ã€{factor_name}ã€‘å› å­ä¸­ extra_data_dict é…ç½®")

        for data_source in factor.extra_data_dict.keys():
            from config import data_source_dict
            if data_source not in data_source_dict:
                raise Exception(f"æœªæ‰¾åˆ° extra_data_dict é…ç½®çš„æ•°æ®æºï¼š{data_source}")

    print(f'âœ… {factors} å› å­é…ç½®æ£€æŸ¥é€šè¿‡')
